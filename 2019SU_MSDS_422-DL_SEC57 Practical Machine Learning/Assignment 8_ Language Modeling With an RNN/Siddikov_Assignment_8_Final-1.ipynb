{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In this assignment, we are asked to explore the TensorFlow's Recurrent Neural Network (RNN) algorithm for predicting movie review sentiments. Each of the reviews has either a positive or negative sentiment, a thumbs up or a thumbs down, associated with the content. \n",
    "\n",
    "For this problem we used a set of two pre-trained textual movie reviews embeddings from GloVe, short for Global Vectors for Word Representation, developed by researchers at Stanford University, to transform the written language content into its numeric representation. These encodings allow us to transform a sequence of words, or a sentence, into a sequence of numeric vectors of which we can derive mathematical models. We also used two vocabulary sizes. \n",
    "\n",
    "We need to evaluate the four language models to classify movie reviews into positives and negatives and make recommendations to management. If the most critical customer messages can be identified, then customer support personnel can be assigned to contact those customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# ignore all future warnings\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import base packages into the namespace for this program\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import chakin  \n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# import base packages into the namespace for this program\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os  # operating system functions\n",
    "import os.path  # for manipulation of file path names\n",
    "import re  # regular expressions\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 9999\n",
    "# chakin.search(lang='English')  # lists available indices in English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Model Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(filename, EVOCABSIZE, n_neurons, n_epochs):\n",
    "    \n",
    "    # To make output stable across runs\n",
    "    def reset_graph(seed = RANDOM_SEED):\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    REMOVE_STOPWORDS = False  # no stopword removal \n",
    "    EVOCABSIZE = EVOCABSIZE  # specify desired size of pre-defined embedding vocabulary \n",
    "    \n",
    "    # Select the pre-defined embeddings source        \n",
    "    # Define vocabulary size for the language model    \n",
    "    # Create a word_to_embedding_dict \n",
    "    embeddings_directory = 'embeddings/gloVe.6B'\n",
    "    filename = filename ## argument\n",
    "    embeddings_filename = os.path.join(embeddings_directory, filename)\n",
    "\n",
    "    # Creates the Python defaultdict dictionary word_to_embedding_dict\n",
    "    # for the requested pre-trained word embeddings\n",
    "    def load_embedding_from_disks(embeddings_filename, with_indexes=True):\n",
    "        if with_indexes:\n",
    "            word_to_index_dict = dict()\n",
    "            index_to_embedding_array = []\n",
    "        else:\n",
    "            word_to_embedding_dict = dict()\n",
    "        with open(embeddings_filename, 'r', encoding='utf-8') as embeddings_file:\n",
    "            for (i, line) in enumerate(embeddings_file):\n",
    "                split = line.split(' ')\n",
    "                word = split[0]\n",
    "                representation = split[1:]\n",
    "                representation = np.array(\n",
    "                    [float(val) for val in representation]\n",
    "                )\n",
    "                if with_indexes:\n",
    "                    word_to_index_dict[word] = i\n",
    "                    index_to_embedding_array.append(representation)\n",
    "                else:\n",
    "                    word_to_embedding_dict[word] = representation\n",
    "\n",
    "        # Empty representation for unknown words.\n",
    "        _WORD_NOT_FOUND = [0.0] * len(representation)\n",
    "        if with_indexes:\n",
    "            _LAST_INDEX = i + 1\n",
    "            word_to_index_dict = defaultdict(\n",
    "                lambda: _LAST_INDEX, word_to_index_dict)\n",
    "            index_to_embedding_array = np.array(\n",
    "                index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "            return word_to_index_dict, index_to_embedding_array\n",
    "        else:\n",
    "            word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "            return word_to_embedding_dict\n",
    "\n",
    "    # Loading embeddings from embeddings_filename\n",
    "    word_to_index, index_to_embedding = \\\n",
    "        load_embedding_from_disks(embeddings_filename, with_indexes=True)\n",
    "\n",
    "    vocab_size, embedding_dim = index_to_embedding.shape\n",
    "\n",
    "    # Define vocabulary size for the language model    \n",
    "    # To reduce the size of the vocabulary to the n most frequently used words\n",
    "    def default_factory():\n",
    "        return EVOCABSIZE  # last/unknown-word row in limited_index_to_embedding\n",
    "    \n",
    "    # dictionary has the items() function, returns list of (key, value) tuples\n",
    "    limited_word_to_index = defaultdict(default_factory, \\\n",
    "        {k: v for k, v in word_to_index.items() if v < EVOCABSIZE})\n",
    "\n",
    "    # Select the first EVOCABSIZE rows to the index_to_embedding\n",
    "    limited_index_to_embedding = index_to_embedding[0:EVOCABSIZE,:]\n",
    "    \n",
    "    # Set the unknown-word row to be all zeros as previously\n",
    "    limited_index_to_embedding = np.append(limited_index_to_embedding, \n",
    "        index_to_embedding[index_to_embedding.shape[0] - 1, :].\\\n",
    "            reshape(1,embedding_dim), \n",
    "        axis = 0)\n",
    "\n",
    "    # Delete large numpy array to clear some CPU RAM\n",
    "    del index_to_embedding\n",
    "\n",
    "    # Utility function to get file names within a directory\n",
    "    def listdir_no_hidden(path):\n",
    "        start_list = os.listdir(path)\n",
    "        end_list = []\n",
    "        for file in start_list:\n",
    "            if (not file.startswith('.')):\n",
    "                end_list.append(file)\n",
    "        return(end_list)\n",
    "    \n",
    "    # define list of codes to be dropped from document\n",
    "    # carriage-returns, line-feeds, tabs\n",
    "    codelist = ['\\r', '\\n', '\\t']   \n",
    "\n",
    "    # We will not remove stopwords in this exercise because they are\n",
    "    # important to keeping sentences intact\n",
    "    if REMOVE_STOPWORDS:\n",
    "        more_stop_words = ['cant','didnt','doesnt','dont','goes','isnt','hes',\\\n",
    "            'shes','thats','theres','theyre','wont','youll','youre','youve', 'br'\\\n",
    "            've', 're', 'vs'] \n",
    "        some_proper_nouns_to_remove = ['dick','ginger','hollywood','jack',\\\n",
    "            'jill','john','karloff','kudrow','orson','peter','tcm','tom',\\\n",
    "            'toni','welles','william','wolheim','nikita']\n",
    "        # start with the initial list and add to it for movie text work \n",
    "        stoplist = nltk.corpus.stopwords.words('english') + more_stop_words +\\\n",
    "            some_proper_nouns_to_remove\n",
    "\n",
    "    def text_parse(string):\n",
    "        # replace non-alphanumeric with space \n",
    "        temp_string = re.sub('[^a-zA-Z]', '  ', string)    \n",
    "        # replace codes with space\n",
    "        for i in range(len(codelist)):\n",
    "            stopstring = ' ' + codelist[i] + '  '\n",
    "            temp_string = re.sub(stopstring, '  ', temp_string)      \n",
    "        # replace single-character words with space\n",
    "        temp_string = re.sub('\\s.\\s', ' ', temp_string)   \n",
    "        # convert uppercase to lowercase\n",
    "        temp_string = temp_string.lower()    \n",
    "        if REMOVE_STOPWORDS:\n",
    "            # replace selected character strings/stop-words with space\n",
    "            for i in range(len(stoplist)):\n",
    "                stopstring = ' ' + str(stoplist[i]) + ' '\n",
    "                temp_string = re.sub(stopstring, ' ', temp_string)        \n",
    "        # replace multiple blank characters with one blank character\n",
    "        temp_string = re.sub('\\s+', ' ', temp_string)    \n",
    "        return(temp_string)    \n",
    "\n",
    "    # gather data for 500 negative movie reviews\n",
    "    dir_name = 'run-jump-start-rnn-sentiment-big-v002/movie-reviews-negative'\n",
    "    filenames = listdir_no_hidden(path=dir_name)\n",
    "    num_files = len(filenames)\n",
    "\n",
    "    for i in range(len(filenames)):\n",
    "        file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "        assert file_exists\n",
    "\n",
    "    def read_data(filename):\n",
    "        with open(filename, encoding='utf-8') as f:\n",
    "            data = tf.compat.as_str(f.read())\n",
    "            data = data.lower()\n",
    "            data = text_parse(data)\n",
    "            data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "        return data\n",
    "    negative_documents = []\n",
    "\n",
    "    for i in range(num_files):\n",
    "        words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "        negative_documents.append(words)\n",
    "\n",
    "    # gather data for 500 positive movie reviews\n",
    "    dir_name = 'run-jump-start-rnn-sentiment-big-v002/movie-reviews-positive'  \n",
    "    filenames = listdir_no_hidden(path=dir_name)\n",
    "    num_files = len(filenames)\n",
    "\n",
    "    for i in range(len(filenames)):\n",
    "        file_exists = os.path.isfile(os.path.join(dir_name, filenames[i]))\n",
    "        assert file_exists\n",
    "\n",
    "    def read_data(filename):\n",
    "        with open(filename, encoding='utf-8') as f:\n",
    "            data = tf.compat.as_str(f.read())\n",
    "            data = data.lower()\n",
    "            data = text_parse(data)\n",
    "            data = TreebankWordTokenizer().tokenize(data)  # The Penn Treebank\n",
    "        return data\n",
    "    positive_documents = []\n",
    "\n",
    "    for i in range(num_files):\n",
    "        words = read_data(os.path.join(dir_name, filenames[i]))\n",
    "        positive_documents.append(words)\n",
    "\n",
    "    max_review_length = 0  # initialize\n",
    "    for doc in negative_documents:\n",
    "        max_review_length = max(max_review_length, len(doc))    \n",
    "    for doc in positive_documents:\n",
    "        max_review_length = max(max_review_length, len(doc)) \n",
    "\n",
    "    min_review_length = max_review_length  # initialize\n",
    "    for doc in negative_documents:\n",
    "        min_review_length = min(min_review_length, len(doc))    \n",
    "    for doc in positive_documents:\n",
    "        min_review_length = min(min_review_length, len(doc)) \n",
    "\n",
    "    from itertools import chain\n",
    "    documents = []\n",
    "    for doc in negative_documents:\n",
    "        doc_begin = doc[0:20]\n",
    "        doc_end = doc[len(doc) - 20: len(doc)]\n",
    "        documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "    for doc in positive_documents:\n",
    "        doc_begin = doc[0:20]\n",
    "        doc_end = doc[len(doc) - 20: len(doc)]\n",
    "        documents.append(list(chain(*[doc_begin, doc_end])))    \n",
    "\n",
    "    # create list of lists of lists for embeddings\n",
    "    embeddings = []    \n",
    "    for doc in documents:\n",
    "        embedding = []\n",
    "        for word in doc:\n",
    "            embedding.append(limited_index_to_embedding[limited_word_to_index[word]]) \n",
    "        embeddings.append(embedding)\n",
    " \n",
    "    # Check on the embeddings list of list of lists \n",
    "    # Show the first word in the first document\n",
    "    test_word = documents[0][0]    \n",
    "\n",
    "    # Show the seventh word in the tenth document\n",
    "    test_word = documents[6][9]    \n",
    "\n",
    "    # Show the last word in the last document\n",
    "    test_word = documents[999][39]    \n",
    "\n",
    "    # Make embeddings a numpy array for use in an RNN \n",
    "    # Create training and test sets with Scikit Learn\n",
    "    embeddings_array = np.array(embeddings)\n",
    "\n",
    "    # Define the labels to be used 500 negative (0) and 500 positive (1)\n",
    "    thumbs_down_up = np.concatenate((np.zeros((500), dtype = np.int32), \n",
    "                          np.ones((500), dtype = np.int32)), axis = 0)\n",
    "\n",
    "    # Scikit Learn for random splitting of the data  \n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Random splitting of the data in to training (80%) and test (20%)  \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(embeddings_array, thumbs_down_up, test_size=0.20, \n",
    "                         random_state = RANDOM_SEED)\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    n_steps = embeddings_array.shape[1]  # number of words per document \n",
    "    n_inputs = embeddings_array.shape[2]  # dimension of  pre-trained embeddings\n",
    "    ##n_neurons = 20  # analyst specified number of neurons\n",
    "    n_outputs = 2  # thumbs-down or thumbs-up\n",
    "\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "    y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "\n",
    "    logits = tf.layers.dense(states, n_outputs)\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    #n_epochs = 50\n",
    "    batch_size = 100\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs):\n",
    "            for iteration in range(y_train.shape[0] // batch_size):          \n",
    "                X_batch = X_train[iteration*batch_size:(iteration + 1)*batch_size,:]\n",
    "                y_batch = y_train[iteration*batch_size:(iteration + 1)*batch_size]\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "    \n",
    "    # performance score table\n",
    "    col_names=['Word Vector', 'Vocabulary Size', 'Neurons','Epochs',\\\n",
    "               'Training Set Accuracy', 'Test Set Accuracy']\n",
    "    perf=pd.DataFrame([filename, EVOCABSIZE, n_neurons, n_epochs,\\\n",
    "                       np.round(acc_train, 2), np.round(acc_test, 2)],\\\n",
    "                      columns=[''], index=col_names).T\n",
    "    return perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Vector</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.50d.txt</td>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Vector Vocabulary Size Neurons Epochs Training Set Accuracy  \\\n",
       "  glove.6B.50d.txt           10000      20     50                   0.8   \n",
       "\n",
       " Test Set Accuracy  \n",
       "              0.68  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_1 = run_model(filename = 'glove.6B.50d.txt', EVOCABSIZE = 10000, \n",
    "          n_neurons = 20, n_epochs = 50)\n",
    "perf_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Vector</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.100d.txt</td>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word Vector Vocabulary Size Neurons Epochs Training Set Accuracy  \\\n",
       "  glove.6B.100d.txt           10000      20     50                  0.92   \n",
       "\n",
       " Test Set Accuracy  \n",
       "              0.66  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_2 = run_model(filename = 'glove.6B.100d.txt', EVOCABSIZE = 10000, \n",
    "          n_neurons = 20, n_epochs = 50)\n",
    "perf_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Vector</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.50d.txt</td>\n",
       "      <td>30000</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Vector Vocabulary Size Neurons Epochs Training Set Accuracy  \\\n",
       "  glove.6B.50d.txt           30000      20     50                  0.82   \n",
       "\n",
       " Test Set Accuracy  \n",
       "               0.7  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_3 = run_model(filename = 'glove.6B.50d.txt', EVOCABSIZE = 30000, \n",
    "          n_neurons = 20, n_epochs = 50)\n",
    "perf_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Vector</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.100d.txt</td>\n",
       "      <td>30000</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word Vector Vocabulary Size Neurons Epochs Training Set Accuracy  \\\n",
       "  glove.6B.100d.txt           30000      20     50                  0.95   \n",
       "\n",
       " Test Set Accuracy  \n",
       "              0.66  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_4 = run_model(filename = 'glove.6B.100d.txt', EVOCABSIZE = 30000, \n",
    "          n_neurons = 20, n_epochs = 50)\n",
    "perf_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Vector</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.50d.txt</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Vector Vocabulary Size Neurons Epochs Training Set Accuracy  \\\n",
       "  glove.6B.50d.txt           10000     100     50                     1   \n",
       "\n",
       " Test Set Accuracy  \n",
       "              0.54  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_5 = run_model(filename = 'glove.6B.50d.txt', EVOCABSIZE = 10000, \n",
    "          n_neurons = 100, n_epochs = 50)\n",
    "perf_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Vector</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.50d.txt</td>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Vector Vocabulary Size Neurons Epochs Training Set Accuracy  \\\n",
       "  glove.6B.50d.txt           10000      20    100                  0.88   \n",
       "\n",
       " Test Set Accuracy  \n",
       "              0.69  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_6 = run_model(filename = 'glove.6B.50d.txt', EVOCABSIZE = 10000, \n",
    "          n_neurons = 20, n_epochs = 100)\n",
    "perf_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Vector</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.50d.txt</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Vector Vocabulary Size Neurons Epochs Training Set Accuracy  \\\n",
       "  glove.6B.50d.txt           10000     100    100                     1   \n",
       "\n",
       " Test Set Accuracy  \n",
       "              0.56  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_7 = run_model(filename = 'glove.6B.50d.txt', EVOCABSIZE = 10000, \n",
    "          n_neurons = 100, n_epochs = 100)\n",
    "perf_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Vector</th>\n",
       "      <th>Vocabulary Size</th>\n",
       "      <th>Neurons</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Training Set Accuracy</th>\n",
       "      <th>Test Set Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.50d.txt</td>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.100d.txt</td>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.50d.txt</td>\n",
       "      <td>30000</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.100d.txt</td>\n",
       "      <td>30000</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.50d.txt</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.50d.txt</td>\n",
       "      <td>10000</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>glove.6B.50d.txt</td>\n",
       "      <td>10000</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word Vector Vocabulary Size Neurons Epochs Training Set Accuracy  \\\n",
       "   glove.6B.50d.txt           10000      20     50                   0.8   \n",
       "  glove.6B.100d.txt           10000      20     50                  0.92   \n",
       "   glove.6B.50d.txt           30000      20     50                  0.82   \n",
       "  glove.6B.100d.txt           30000      20     50                  0.95   \n",
       "   glove.6B.50d.txt           10000     100     50                     1   \n",
       "   glove.6B.50d.txt           10000      20    100                  0.88   \n",
       "   glove.6B.50d.txt           10000     100    100                     1   \n",
       "\n",
       " Test Set Accuracy  \n",
       "              0.68  \n",
       "              0.66  \n",
       "               0.7  \n",
       "              0.66  \n",
       "              0.54  \n",
       "              0.69  \n",
       "              0.56  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([perf_1, perf_2, perf_3, perf_4, perf_5, perf_6, perf_7], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we are asked to find the highest possible accuracy in movie review classification (negative vs. positive). The highest training accuracy was 100%, and testing accuracy was 70%. The recommendation is to use model 3 – the Recurrent Neural Network (RNN) model. When we increased the vocabulary size of model 3 from 1,000 to 3,000, the test set accuracy went up by 2%. When we increased the word vector glove.6B.50d to glove.6B.100, the training accuracy went up significantly, but testing accuracy went down due to the overfitting. Increased the epochs (number of iterations to train the model over the entire dataset) from 50 to 100 did not move the needle. When we increased the neurons from 20 to 100, the training accuracy went up to the highest, 100%, but the testing accuracy went down to the lowest, around 55% due to the overfitting. We can save time and money by not increasing the neurons and epochs from their default values. With this assignment, we were able to scratch the surface of NLP, text mining. We can tune other hyperparameters to improve the model further. We would recommend it for a high-level classification system for assessing the customer's sentiment of written product reviews or customer experience surveys.  A system which can read sentiments and sort reviews into negative and positive would be most relevant to the customer services function. Model 3 which has lower word vector (glove.6B.50d), larger vocabulary size (3,000) with default neurons (20), and iteration epochs (50) needed to make an automated customer support system that is capable of identifying negative customer feelings. Once data scientists find the right hyperparameters to achieve the highest testing accuracy, they can implement automate the customer reviews, which can be more useful in a customer service function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
